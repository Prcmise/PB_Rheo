{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f62196b",
   "metadata": {},
   "source": [
    "\n",
    "# Rheology Prediction (G′ / G″) — Colab Baseline (v2: renamed columns + quality weighting)\n",
    "**Changes in v2**  \n",
    "- Column names updated:  \n",
    "  - `Temperature_C` → **`Temp_C`**  \n",
    "  - `Frequency_rad_s` → **`Freq_rad_s`**  \n",
    "  - `Gprime_Pa` → **`G1_Pa`**  \n",
    "  - `Gdoubleprime_Pa` → **`G2_Pa`**  \n",
    "- After computing `tan_delta = G2_Pa / G1_Pa`, rows with **tan_delta > 1000** are flagged as suspicious and assigned **10× lower sample weight** for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e6958",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Environment & Data Upload\n",
    "Choose one:\n",
    "1) Upload local CSV (`files.upload`)  \n",
    "2) Mount Google Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option A: Upload local CSV from your computer\n",
    "from google.colab import files\n",
    "import io, pandas as pd\n",
    "\n",
    "uploaded = files.upload()  # Choose your CSV file (e.g., PB_Data.csv)\n",
    "csv_name = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(io.BytesIO(uploaded[csv_name]))\n",
    "print(\"Loaded:\", csv_name, \"shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option B: Mount Google Drive and read from a path\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# import pandas as pd\n",
    "# csv_path = \"/content/drive/MyDrive/your_folder/PB_Data.csv\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "# print(\"Loaded:\", csv_path, \"shape:\", df.shape)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10117a43",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Basic Schema Checks (v2)\n",
    "Expected columns (case-sensitive):\n",
    "- `Sample_ID` (string)\n",
    "- `Length_nm`, `Width_nm` (numeric)\n",
    "- `Temp_C` (numeric)\n",
    "- `Freq_rad_s` (numeric, >0)\n",
    "- `G1_Pa`, `G2_Pa` (numeric, >0)\n",
    "- `tan_delta` (optional; if absent, computed as `G2_Pa / G1_Pa`)\n",
    "Also, rows with `tan_delta > 1000` will be down-weighted by 10× during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "required_cols = ['Sample_ID','Length_nm','Width_nm','Temp_C','Freq_rad_s','G1_Pa','G2_Pa']\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Coerce numeric types\n",
    "for col in ['Length_nm','Width_nm','Temp_C','Freq_rad_s','G1_Pa','G2_Pa']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop rows with critical NaNs\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['Sample_ID','Length_nm','Width_nm','Temp_C','Freq_rad_s','G1_Pa','G2_Pa'])\n",
    "after = len(df)\n",
    "print(f\"Dropped {before-after} rows due to NaNs in critical columns. Remaining:\", after)\n",
    "\n",
    "# Basic positivity checks for logs\n",
    "assert (df['G1_Pa'] > 0).all(), \"G1_Pa must be > 0 for log transforms\"\n",
    "assert (df['G2_Pa'] > 0).all(), \"G2_Pa must be > 0 for log transforms\"\n",
    "assert (df['Freq_rad_s'] > 0).all(), \"Freq_rad_s must be > 0 for log transforms\"\n",
    "\n",
    "# Compute tan_delta if not provided\n",
    "if 'tan_delta' not in df.columns:\n",
    "    df['tan_delta'] = df['G2_Pa'] / df['G1_Pa']\n",
    "\n",
    "# Quality flag & sample weights\n",
    "df['flag_suspicious'] = df['tan_delta'] > 1000\n",
    "df['sample_weight'] = np.where(df['flag_suspicious'], 0.1, 1.0)\n",
    "\n",
    "print(\"Suspicious rows (tan_delta > 1000):\", int(df['flag_suspicious'].sum()))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a2097",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Feature Engineering\n",
    "- `Aspect = Length_nm / Width_nm`\n",
    "- `log10_freq = log10(Freq_rad_s)`\n",
    "- Targets in log space: `log10_Gp = log10(G1_Pa)`, `log10_Gpp = log10(G2_Pa)`\n",
    "- Interaction: `T_x_logf = Temp_C * log10_freq`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.copy()\n",
    "df['Aspect'] = df['Length_nm'] / df['Width_nm']\n",
    "df['log10_freq'] = np.log10(df['Freq_rad_s'])\n",
    "df['log10_Gp'] = np.log10(df['G1_Pa'])\n",
    "df['log10_Gpp'] = np.log10(df['G2_Pa'])\n",
    "df['T_x_logf'] = df['Temp_C'] * df['log10_freq']\n",
    "\n",
    "feature_cols = ['Length_nm','Width_nm','Aspect','Temp_C','log10_freq','T_x_logf']\n",
    "target_cols  = ['log10_Gp','log10_Gpp']\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_cols].values\n",
    "groups = df['Sample_ID'].values\n",
    "weights = df['sample_weight'].values\n",
    "\n",
    "X.shape, y.shape, weights.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84fd07",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Group-Aware Cross-Validation with Sample Weights\n",
    "We group by `Sample_ID` to evaluate generalization to unseen materials.  \n",
    "Suspicious rows (tan_delta > 1000) receive 10× lower weight during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dfe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )))\n",
    "])\n",
    "\n",
    "cv_metrics = []\n",
    "fold = 0\n",
    "for train_idx, valid_idx in gkf.split(X, y, groups):\n",
    "    fold += 1\n",
    "    X_tr, X_va = X[train_idx], X[valid_idx]\n",
    "    y_tr, y_va = y[train_idx], y[valid_idx]\n",
    "    w_tr = weights[train_idx]\n",
    "\n",
    "    pipe.fit(X_tr, y_tr, model__sample_weight=w_tr)\n",
    "    y_pred = pipe.predict(X_va)\n",
    "\n",
    "    # Evaluate in log space\n",
    "    mae_log = mean_absolute_error(y_va, y_pred, multioutput='raw_values')\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_va, y_pred, multioutput='raw_values'))\n",
    "    r2_log = [r2_score(y_va[:,0], y_pred[:,0]), r2_score(y_va[:,1], y_pred[:,1])]\n",
    "\n",
    "    # Linear space\n",
    "    y_va_lin = 10**y_va\n",
    "    y_pred_lin = 10**y_pred\n",
    "    mae_lin = mean_absolute_error(y_va_lin, y_pred_lin, multioutput='raw_values')\n",
    "    rmse_lin = np.sqrt(mean_squared_error(y_va_lin, y_pred_lin, multioutput='raw_values'))\n",
    "\n",
    "    cv_metrics.append({\n",
    "        'fold': fold,\n",
    "        'MAE_log10_Gp': mae_log[0],\n",
    "        'MAE_log10_Gpp': mae_log[1],\n",
    "        'RMSE_log10_Gp': rmse_log[0],\n",
    "        'RMSE_log10_Gpp': rmse_log[1],\n",
    "        'R2_log10_Gp': r2_log[0],\n",
    "        'R2_log10_Gpp': r2_log[1],\n",
    "        'MAE_Gp(Pa)': mae_lin[0],\n",
    "        'MAE_Gpp(Pa)': mae_lin[1],\n",
    "        'RMSE_Gp(Pa)': rmse_lin[0],\n",
    "        'RMSE_Gpp(Pa)': rmse_lin[1],\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_metrics)\n",
    "cv_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a6c64",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Residual Diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56064221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pipe.fit(X, y, model__sample_weight=weights)\n",
    "y_hat = pipe.predict(X)\n",
    "\n",
    "# Residuals for log10(G′)\n",
    "res_gp = y[:,0] - y_hat[:,0]\n",
    "plt.figure()\n",
    "plt.scatter(y_hat[:,0], res_gp, s=8)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicted log10(G′)\")\n",
    "plt.ylabel(\"Residual (true - pred)\")\n",
    "plt.title(\"Residuals for log10(G′)\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals for log10(G″)\n",
    "res_gpp = y[:,1] - y_hat[:,1]\n",
    "plt.figure()\n",
    "plt.scatter(y_hat[:,1], res_gpp, s=8)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicted log10(G″)\")\n",
    "plt.ylabel(\"Residual (true - pred)\")\n",
    "plt.title(\"Residuals for log10(G″)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8a85b",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Train Final Model & Export Artifacts\n",
    "Saves: `model.pkl` (pipeline), `feature_info.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d633c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib, json\n",
    "\n",
    "pipe.fit(X, y, model__sample_weight=weights)\n",
    "joblib.dump(pipe, \"model.pkl\")\n",
    "feature_info = {\n",
    "    \"feature_cols\": ['Length_nm','Width_nm','Aspect','Temp_C','log10_freq','T_x_logf'],\n",
    "    \"target_cols\": ['log10_Gp','log10_Gpp'],\n",
    "    \"log_targets\": True,\n",
    "    \"note\": \"Inputs expect Sample_ID, Length_nm, Width_nm, Temp_C, Freq_rad_s. Internally builds Aspect/log10_freq/T_x_logf. Suspicious rows (tan_delta>1000) down-weighted by 10x.\"\n",
    "}\n",
    "with open(\"feature_info.json\",\"w\") as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(\"Saved: model.pkl, feature_info.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582a360",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Predict on New Designs\n",
    "Prepare a CSV with columns: `Sample_ID, Length_nm, Width_nm, Temp_C, Freq_rad_s`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b290d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example prediction input template (v2)\n",
    "import pandas as pd\n",
    "pred_in = pd.DataFrame({\n",
    "    \"Sample_ID\": [\"N1\",\"N1\",\"N2\"],\n",
    "    \"Length_nm\": [120,120,150],\n",
    "    \"Width_nm\": [30,30,35],\n",
    "    \"Temp_C\": [25,40,25],\n",
    "    \"Freq_rad_s\": [1.0, 10.0, 0.1],\n",
    "})\n",
    "pred_in.to_csv(\"prediction_input_example_v2.csv\", index=False)\n",
    "pred_in.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load artifacts and predict (v2)\n",
    "import joblib, numpy as np, pandas as pd\n",
    "\n",
    "pipe = joblib.load(\"model.pkl\")\n",
    "\n",
    "inp = pd.read_csv(\"prediction_input_example_v2.csv\")\n",
    "inp['Aspect'] = inp['Length_nm'] / inp['Width_nm']\n",
    "inp['log10_freq'] = np.log10(inp['Freq_rad_s'])\n",
    "inp['T_x_logf'] = inp['Temp_C'] * inp['log10_freq']\n",
    "\n",
    "X_new = inp[['Length_nm','Width_nm','Aspect','Temp_C','log10_freq','T_x_logf']].values\n",
    "y_pred_log = pipe.predict(X_new)\n",
    "y_pred = 10**y_pred_log\n",
    "\n",
    "out = inp.copy()\n",
    "out['G1_Pa_pred'] = y_pred[:,0]\n",
    "out['G2_Pa_pred'] = y_pred[:,1]\n",
    "out['tan_delta_pred'] = out['G2_Pa_pred'] / out['G1_Pa_pred']\n",
    "out.to_csv(\"prediction_output_v2.csv\", index=False)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d7890c",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Optional) Hyperparameter Search (with weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65955414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "base_rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=MultiOutputRegressor(base_rf),\n",
    "    param_distributions={\n",
    "        \"estimator__n_estimators\": randint(200, 800),\n",
    "        \"estimator__max_depth\": randint(3, 20),\n",
    "        \"estimator__min_samples_split\": randint(2, 20),\n",
    "        \"estimator__min_samples_leaf\": randint(1, 10),\n",
    "    },\n",
    "    n_iter=20,\n",
    "    cv=GroupKFold(n_splits=5).split(X, y, groups),\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X, y, estimator__sample_weight=weights)\n",
    "print(\"Best params:\", search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50ecf7",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (Optional) Uncertainty Estimate via Tree Std (with weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "rf_gp = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "rf_gpp = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "\n",
    "rf_gp.fit(X_scaled, y[:,0], sample_weight=weights)\n",
    "rf_gpp.fit(X_scaled, y[:,1], sample_weight=weights)\n",
    "\n",
    "def pred_with_std(model, Xs):\n",
    "    preds = np.vstack([t.predict(Xs) for t in model.estimators_])\n",
    "    return preds.mean(axis=0), preds.std(axis=0)\n",
    "\n",
    "# Example using prediction_input_example_v2.csv\n",
    "inp = pd.read_csv(\"prediction_input_example_v2.csv\")\n",
    "inp['Aspect'] = inp['Length_nm'] / inp['Width_nm']\n",
    "inp['log10_freq'] = np.log10(inp['Freq_rad_s'])\n",
    "inp['T_x_logf'] = inp['Temp_C'] * inp['log10_freq']\n",
    "Xs = scaler.transform(inp[['Length_nm','Width_nm','Aspect','Temp_C','log10_freq','T_x_logf']].values)\n",
    "\n",
    "m_gp, s_gp = pred_with_std(rf_gp, Xs)\n",
    "m_gpp, s_gpp = pred_with_std(rf_gpp, Xs)\n",
    "\n",
    "unc = inp.copy()\n",
    "unc['log10_Gp_mean'] = m_gp\n",
    "unc['log10_Gp_std'] = s_gp\n",
    "unc['log10_Gpp_mean'] = m_gpp\n",
    "unc['log10_Gpp_std'] = s_gpp\n",
    "\n",
    "unc['Gp_Pa_mean'] = 10**unc['log10_Gp_mean']\n",
    "unc['Gp_Pa_low']  = 10**(unc['log10_Gp_mean'] - 2*s_gp)\n",
    "unc['Gp_Pa_high'] = 10**(unc['log10_Gp_mean'] + 2*s_gp)\n",
    "\n",
    "unc['Gpp_Pa_mean'] = 10**unc['log10_Gpp_mean']\n",
    "unc['Gpp_Pa_low']  = 10**(unc['log10_Gpp_mean'] - 2*s_gpp)\n",
    "unc['Gpp_Pa_high'] = 10**(unc['log10_Gpp_mean'] + 2*s_gpp)\n",
    "\n",
    "unc\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
